
\documentclass[bachelor]{thesis-uestc}

\title{基于大语言模型增强的视觉语言导航研究}{TODO}
\author{李皓}{Li Hao}
\advisor{王鹏\chinesespace 教授}{Dr. Peng Wang}
\school{计算机科学与工程学院（网安学院）}{TODO}
\major{数据科学与大数据技术}{TODO}
\studentnumber{2020080903009}

\begin{document}
\makecover

\begin{chineseabstract}


\chinesekeyword{}
\end{chineseabstract}

\begin{englishabstract}

\englishkeyword{}
\end{englishabstract}

\thesistableofcontents

\chapter{绪\hspace{6pt}论}

\section{研究工作的背景与意义}


视觉语言导航（Version-and-Language Navigation，VLN）作为具身智能（Embodied Agent）的前沿研究方向，涉及智能体与人类的交互通信，其中人类发布指令，智能体理解指令，观察环境，导航或与环境互动，最终完成任务。

VLN 任务将计算机视觉，自然语言处理等深度学习的研究方向结合在一起，提升了智能体对于视觉和语言信息的联合感知能力，能有效拓宽智能体的应用场景。在导航过程中，智能体处于特定环境中，收到指令，并通过某种形式探索环境，规划路线，最终完成导航任务。自然语言指令贴合人和智能体交互的一般场景，而视觉信息帮助智能体交互时理解周围环境。

在日常任务处理，或者是抢险救灾等情况下，能够自主进行导航任务的机器人将有助于解放人的劳动，保障人的生命安全。


\section{视觉语言导航的国内外研究历史与现状}
为了将自然语言指令和陌生的视觉环境以及行动过程联系起来，Anderson 等人利用 MatterPort3D 环境构建出了 R2R 数据集。Matterport3D 数据集在90个不同风格的建筑下采样了10800个RGB和深度图像，提供了丰富的真实场景信息。R2R 数据集由文本指令和对应的视点轨迹构成，平均每条指令包含29个单词，视点和Matterport3D中的采样点相对应。此外，为了模拟智能体同环境交互的行为，该工作还构建了相应的执行模拟器 MatterPort3D Simulator。

此后在该数据集的基础上，更多相关的室内导航数据集被提出。在 R2R 的基础上衍生出若干变体，如 Room-for-Room，Room-across-Room，Lanndmark-R2R等，在指令轨迹、多文化环境和地标信息的角度扩展了原始数据集。一些数据集将眼光从室内移向室外，如 TOUCHDOWN 数据集利用纽约市的街景信息，将室内环境扩展到室外。在任务方面，ALFRED和REVERIE数据集引入了和物体之间的交互任务，不仅仅要求智能体导航到目标所在的地点，还需要和指定的额物体进行交互（拾取、互动等），有效扩展了智能体的行动空间，更加贴近应用场景。

随着计算机视觉和自然语言处理技术的一步步发展，智能体的导航策略模型也在同步发展。在视觉提取方面，特征提取方法由 ResNet 演变到 Vision Transformer 等模型；指令理解方面，基于 RNN 的方法逐渐被基于 BERT 的方法取代。




\section{本文的主要贡献与创新}

\section{本论文的结构安排}
本文的章节结构安排如下：


\chapter{视觉语言导航模型研究}

在本章节，我们将详细探讨视觉语言导航的研究进展，包括、数据集，前沿的研究方法等。首先将介绍本研究使用到的视觉语言导航数据集R2R，REVERIE，以及FG-R2R，包括其特点和差异，其次将介绍视觉语言导航任务的衡量标准，最后将介绍近年来在 REVERIE 数据集上取得较好性能的方法。

\section{数据集}

本研究主要就R2R，REVERIE以及FG-R2R数据集进行研究。R2R 为视觉语言导航任务提供了详细的指令，REVERIE 数据集和 R2R 共享相同的轨迹和环境，但指令更为精简，需要智能体探索环境。FG-R2R 数据集提供了R2R 指令细粒度的切分，并标记了对应的子指令序列。本研究将利用这三个数据集，探索智能体在REVERIE高级指令下，面对未见场景的导航性能。

\subsection{R2R 数据集}
R2R数据集是首个提出视觉语言导航任务的数据集，数据由详细的指令和指令对应的路径构成。指令由人工标注，详细描述了从起点到终点经过的移动过程，包含地标信息，房间功能，以及关键物体的特征描述。每条指令平均长度为29个词。路径在Matterport3D采集的室内场景构建而来，每条轨迹由离散的导航点构成。在每个离散的导航点处，智能体都能获得当前的全景观察，包含RGB色彩信息以及深度信息。

\textcolor{red}{指令提示样例}

视觉语言导航任务最终希望让智能体在现实场景中导航，完成任务，但受限于成本限制，使用模拟环境进行室内环境导航更可行。R2R 数据集基于 Matterport3D 数据集构建了 Matterport3D simulator ，用于模拟智能体在室内导航场景下的移动。智能体在环境中的姿势由三个维度决定：视点$V$，航向角（heading）和俯仰角决定，在每个姿势下都能获得第一人称RGB-D图像和周围可导航的视点。智能体在环境中只能在相邻视点之间进行移动。

R2R数据集在90个不同的室内场景构成，场景可能存在多层，大部分是私宅，同时包含办公室，教堂等场景。一个场景平均有117个视点，可能覆盖多个楼层或是多个相同类型的房间，如图所示。数据集划分为四个部分，训练集（training set），已见-验证集（valid seen set），未见-验证集（valid unseen set），测试集（test set）。其中已见/未见数据集的区别在于，已见数据集在训练集中的相同场景下构建了新的指令，训练过程中智能体已经见过所处的环境，而未见测试集模拟环境陌生指令陌生的情况，更加贴合测试环境。

\textcolor{red}{室内场景提示样例}

R2R 数据集在导航开始时提供了详细的指导，描述导航动作，智能体需要根据周围环境观察判断当前的导航指令的执行情况，并在合适的时机做出动作，直到指令执行完成后停留在终点。考虑人和智能体交互的过程，人们在提出任务时，往往不会用如此详细的方式给出指令。而 REVERIE 数据集的提出，弥补了高级指令的缺失。

\subsection{REVERIE 数据集}
REVERIE数据集提供了更加简洁直接的高级指令，但在任务上有所不同，R2R要求导航到场景中的某个地点，而REVERIE要求导航到某个不可见的物体处，这需要智能体利用周围环境知识推断物体可能所处的可能位置，并且根据语言指令来准确识别物体。相较于 R2R 数据集而言，REVERIE 数据集的单词数量更少，精简了具体的指引，强调最终的目标位置。因此REVERIE中的指令被认为是高级指令，R2R为细粒度指令。对于同一轨迹下的高级指令和细粒度指令之间的区别，如表所示。

\textcolor{red}{REVERIE 和 R2R 指令对比}

REVERIE 数据集以远程物体作为指令主体，需要智能体理解指令的最终意图，也就是说，需要智能体自主决策导航路径。面对这样的场景下，智能体需要具有规划，记忆，以及回溯的功能。智能体接受指令后需要对全局的导航动作进行规划，而这正是REVERIE数据集没有提供的。在执行每一步之后，智能体都需要记录维护走过的动作，观察到的场景信息，并在下一步动作之前分析状态，做出继续前进或者是回溯的选择。本人将在后续章节对此展开详细叙述。

\subsection{FG-R2R 数据集}
为了更好的研究 R2R 指令的构成，以及指令序列和视觉序列之间的对应关系，Hong 等人提出了 Fine-Grained R2R 数据集（FG-R2R），为R2R数据集提供了细粒度的注释。具体来说，FG-R2R 数据集将原始 R2R 数据集中的指令进行启发式切分，将一条原始指令切分为若干条较短的子指令。这些子指令要么指向某些关键物体或者地标，要么描述关键动作。在获取子指令后，FG-R2R 对环境中的视点做了标注，利用模块化注意力机制匹配子指令片段和对应的导航轨迹片段。FG-R2R 数据集将在子指令微调部分发挥作用，作为学习样例引导LLM从高级指令生成对应的子指令。

\section{视觉语言导航性能指标}

在视觉语言导航任务中，对智能体的评估非常重要。为此，数据集提供了一系列指标，从多角度衡量智能体准确理解指令和在复杂场景下的导航能力。本节将详细介绍这些评估指标，并讨论如何利用这些指标分析智能体的性能。

\begin{itemize}
    \item 成功率（Success Rate，SR）
    
    成功率衡量任务最后成功的比例，智能体移动到目标位置3m以内停止，就视为成功。成功率由式计算。
    \begin{equation}
        \text{SR} = \frac{\text{成功次数}}{\text{总任务次数}}
    \label{sr}
    \end{equation}
    \item 预知成功率 （Oracle Success Rate， OSR）
    
    智能体有时导航到了目标点，却没有停止，而是继续行动，走到了错误的地方。在有外界指导的情况下，智能体可以在成功地点停止。为了衡量这种情况，研究者设置了预知成功率这一指标，统计整个轨迹上预知成功的次数。智能体在一次尝试的轨迹中，有任意视点能观察到三米以内的目标物体，则视为预知成功。预知成功率由式计算。

    \begin{equation}
        \text{OSR} = \frac{\text{预知成功次数}}{\text{总任务次数}}
    \label{OSR}
    \end{equation}


    \item 轨迹长度（Trajectory Length, TL）
    
    智能体在所有尝试过程中的平均轨迹长度，由式计算，
    \begin{equation}
        \text{TL} = \frac{\sum_{i=1}^N l_i}{N}
    \label{tl}
    \end{equation}

    其中 $l_i$ 代表第$i$ 次尝试下从开始到智能体停止的轨迹长度，$N$ 代表总尝试次数。

    \item 路径长度加权成功率 (Success weighted by Path Length, SPL)
    
    SPL 作为一个综合指标，考虑了成功率（SR）以及轨迹长度（TL）。考虑这样的场景，如果仅仅使用成功率衡量智能体的性能，一个显而易见的方法是采用图遍历算法，将室内场景遍历一遍，再选择置信度最高的点作为停止点。这样虽然能带来成功率的数值提升，但是却违背了探索（Exploration）和利用（Exploitation）的平衡关系。我们希望智能体执行指令时，兼具效率和准确性。式为SPL的计算方法：

    \begin{equation}
        \text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \frac{l_i}{\max(p_i, l_i)}
    \label{spl}
    \end{equation}

    其中，\( N \) 是尝试次数，\( S_i \) 是第 \( i \) 次尝试的成功标志（成功为1，失败为0），\( l_i \) 是完成指令的最短路径长度，\( p_i \) 是智能体实际走过的路径长度。
\end{itemize}

上述四个指标反映了导航的性能，成功率相关指标越高越好，轨迹长度则是越短越好。路径长度加权成功率平衡了成功率和路径长度的关系，反映了探索和利用的平衡。后续的工作围绕这些指标，探索不同的模型架构和训练方法，提出了视觉语言导航的新解决思路。

\section{基于学习的方法}

解决视觉语言导航任务需要解决两个关键问题，轨迹记忆和导航规划。智能体在导航过程中需要通过记录走过的路径，维护视点的连通性，先前的工作往往是以RNN，LSTM等网络作为记忆， 这些方法往往将过去的轨迹信息压缩为一个固定大小的向量，可能导致信息丢失。Chen 等人提出的 DUET 模型创新性的构造了一个双尺度的图Transformer 架构，结合了长期的行动规划和细粒度的跨模态理解，通过构建拓扑图维护导航视点，平衡了全局信息和局部接地的性能。该工作在REVERIE 和 SOON 数据集上取得了最好的效果。

\subsection{拓扑图构建}

\textcolor{red}{拓扑图维护什么信息 每一步的更新方式}

DUET 最大的亮点就是利用了拓扑图信息去维护视觉语言导航中的探索过程。在每一次探索中，智能体初始时不知道周围环境的全貌，只能在行动的过程中探索到周围的环境。由于REVERIE 等任务在离散环境下定义，利用图维护导航节点信息高效准确的反映了房间的布局信息。

DUET 构建的拓扑图记录了视觉特征，包含探索过的节点和未探索的节点。在每个时间步$t$，智能体接受到当前节点的图像特征$R_t$ 和对象特征$O_t$ ，这些视觉特征被预先处理好。随后利用多层的transformer来捕捉场景和物体之间的空间关系。其中的自注意力机制如式。

\begin{equation}
    [R'_{t}, O'_{t}] = \text{SelfAttn}([R_{t}, O_{t}]), \\
    \text{SelfAttn}(X) = \text{Softmax}\left( \frac{(XW_{q})(XW_{k})^{T}}{\sqrt{d}} \right) XW_{v}
\label{SelfAttn}
\end{equation}

式中的 transformer 省略了偏差项。经过Transformer 变换后，图像信息和物体信息转为了编码后的嵌入表示。此后，对于当前视点，利用视图和物体的平均池化作为视觉特征，相邻视点处的视觉表示由对应方向上的视图嵌入表示。一个视点可能被多个角度观察到，这些来自不同家督的的视图也被池化。

智能体维护了平均池化结构，作为节点拓扑图的粗粒度表示。这样的作法不仅仅记录了当前所处节点的视觉和物体信息，还为相邻视点的信息提供了视觉特征。在节点的细粒度表示上，节点处的图像特征$R_t$ 和对象特征$O_t$被保留，以支持细粒度的详细推理过程。

\subsection{DUET 模型架构}

在导航过程中，DUET逐步丰富拓扑图节点中的视觉信息。利用拓扑图中的信息，DUET使用编码器输出动作和物体的预测。DUET的主要结构分为两个编码器：粗粒度跨模态编码器和细粒度跨模态编码器，如图所示。

\textcolor{red}{DUET 结构图}

细粒度编码器接收当前的环境信息，输出视点选择和预测物体选择。粗粒度编码器结合拓扑图中$k$ 个视点处的视觉特征，和经过多层Transformer编码后的指令，输出视点预测分数。下面将详细介绍其功能。

\subsubsection{粗粒度编码器}

粗粒度编码器在节点的视觉特征上添加位置编码和导航步骤编码。位置编码是所有节点相对于当前节点的方向和距离，导航步骤编码标记了导航轨迹经过的视点，未经过但是能看见的视点标记为0。这些编码信息维护了节点所处的空间关系，以及导航过程的顺序关系，有助于指令和导航点的接地。此外，在所有节点序列之前，还添加一个停止节点用于最终预测停止分数。

节点嵌入和指令嵌入被送入一个多层图感知跨膜态 Transformer 中，对节点和指令之间的关系进行建模。在该结构中，每一层 Transformer 都包含一个交叉注意力层，用于建模指令和节点之间的关系，以及一个图感知注意力层，用于对环境信息编码。在式2中提出的注意力机制仅仅考虑到了图像之间的相似性，这可能导致临近的节点被优先考虑。为了解决这个问题，DUET 模型提出一种图感知注意力机制，引入距离机制来纠正这一特性。完整的计算过程如下所示。

\begin{equation}
    \begin{aligned}
        \text{GASA}(X) &= \text{Softmax} \left( \frac{(XW_{q})(XW_{k})^{T}}{\sqrt{d}} + M \right) XW_{v}, \\
        M &= EW_{e} + b_{e},
    \end{aligned}
\label{GASA}
\end{equation}

其中$X$代表输入的节点表示，包含视觉，方位等节点信息，$E$ 为拓扑图中的成对距离矩阵，$E$ 和 $b_{e}$ 为可学习参数。在获得每个节点的图感知编码之后，通过两层前馈网络计算得到节点对应的分数，如式.

\begin{equation}
    s^c_i = FFN(\hat{v_i})
\label{}
\end{equation}

$s^c_i $ 代表第$i$个节点的分数，停止分数由$s_0$ 表示。为了避免走回头路，在后续步骤中会将已走过的节点分数屏蔽。粗粒度编码器的分数将会和细粒度编码器分数融合，最终输出导航策略。

\subsubsection{细粒度编码器}

细粒度编码器在局部输出导航选择，基于当前视点下的图像特征$R_t$和对象特征$O_t$ 和指令做出跨模态推理。模块输出局部空间中的导航动作，并在最后时间步定位指令指代的物体。

细粒度编码器首先在视觉特征中添加空间信息，包含相对于起始节点的当前位置，以及相对于当前位置的相邻节点。前者有助于理解整个指令的执行过程，后者便于编码器理解局部动作。为了输出停止动作，还在特征序列添加了停止标记 $r_0$。链接后的特征序列输入到标准的跨模态 Transformer 中，通过式中的方法计算出局部视点选择的分数$s^f_i$。

\subsubsection{动作融合}

粗粒度和细粒度编码器共同参与了导航过程中的路径决策，粗粒度和细粒度的编码DUET 提出一种动态融合的方法合并两个编码器的视点预测。为了统一局部预测中动作空间和全局预测动作空间大小，对于不相邻的视点，采用回溯分数$s_{back}$ 作为细粒度的预测分数。$s_{back}$ 由访问过的邻居节点分数相加得到。此时，细粒度的节点分数用$s^{f'}$ 表示。

\begin{equation}
    s'^{f}_{i} = 
\begin{cases} 
s_{\text{back}}, & \text{if } V_{i} \in V_{t} - \mathcal{N}(V_{t}), \\
s^{f}_{i}, & \text{otherwise}.
\end{cases}
\label{fusion}
\end{equation}

粗粒度和细粒度分数结果利用权重进行融合，权重由编码器中的停止标记经过前馈网络计算得到（式）。

\textcolor{red}{公式}

最终，导航分数由二者融合计算而来（式）。

\textcolor{red}{公式}

\subsection{DUET 模型总结}
DUET 模型在 R2R 和 REVERIE 数据集上取得了当时最好的性能，后续许多研究工作都沿用其结构，并取得了更好的性能。如HM3D-DUET 模型在DUET的基础上扩充新的训练数据，MiC 在 DUET 基础上加入了子指令提示，两个工作都取得了当时的最好性能。

DUET 利用两个尺度的编码器输出导航决策，利用拓扑图维护节点的视觉信息。在粗粒度编码器中考虑了所有节点的特征积累，以及智能体的移动信息，输出粗粒度的导航策略。细粒度的编码器考虑当前视点的邻居节点，以细粒度物体观察和视觉信息决策。两个编码器的架构有利于在全局和局部的范围合成导航选择。

双编码器架构融合指令和观察信息，利用交叉注意力机制将指令和视觉观察接地，这帮助智能体感知当前指令的执行情况。在预测的每一个时间步$t$，智能体都能接收指令进行动作选择，这种灵活的方式也为基于子指令生成的导航解决方案提供了基础。

模型采用动态融合的方法结合两个编码器的导航策略，粗粒度编码器输出全局导航策略，在探索和成功率等方面优于细粒度编码器，而细粒度编码器考虑局部的动作，在提供目标接地的同时，能够精准输出停止动作。采用融合策略结合了两种编码器的优势，也启发了直接生成子动作的方案。

\section{本章小结}

本章介绍了视觉语言导航的数据集，评价指标以及目前性能较好的办法。首先介绍了R2R 数据集，以及衍生出的 REVERIE，FG-R2R 数据集，分析了数据集的特点以及异同。随后介绍了四个导航性能的评价指标，并讨论如何利用这些指标分析智能体的性能。最后介绍了 DUET 模型，该模型在 R2R 数据集和 REVERIE 数据集上取得了最好的性能， 分析其双尺度编码器结构和融合策略，并对模型的亮点进行分析。


\chapter{基于大模型的视觉语言导航研究}

最近，大语言模型（Large Language Model, LLM）已经在视觉语言导航（VLN）任务中发挥作用，一些研究工作已经在尝试这方面的探索。大语言模型在海量文本数据上训练，具有强大的语义理解和文本生成能力。在训练过程中，LLM可能具备室内场景下的一般知识，如”卧室常常和卫生间相连“，有助于在导航过程中提供先验的指导。LLM 还展现出来涌现能力，即使在未经训练的情况下，也能够泛化所学知识，用于解决视觉语言导航这一具身任务。

面对未见环境下的导航任务，LLM 能够展现对周围环境的推断，面对高级的粗粒度指令，LLM 能够推理出导航步骤。LLM强大的能力展现出在视觉语言任务上的潜力，然而目前LLM 在 VLN 任务上仍然面临一系列问题：

\begin{itemize}
    \item 具身任务理解
    
    大语言模型能在未见过的任务下发挥较好的性能，然而，LLM在理解具身导航任务还存在困难。在视觉语言导航中，智能体处于交互环境中，在空间中移动，并实时接受视觉信息和指令信息，输出导航轨迹。LLM 针对人输入的 prompt 进行响应，本质上采用自回归的方式对 prompt 后的内容进行补全。在这个过程中，LLM 并不能直接和环境交互，而是需要解析后进行新的交互。

    \item 幻觉和想象
    
    幻觉指LLM在输入数据的过程中，由于先前训练积累的知识，可能会产生错误或者不真实的信息。在进行导航推理中，需要对房屋布局，室内物体布局，乃至建筑结构布局进行合理的想象。推理和想象之间区别在于，幻觉导致模型误导智能体，而想象帮助智能体执行指令。平衡二者对于增强未见场景的导航性能有十分必要。

    \item 视觉感知
    
    LLM 本身并不具备视觉感知的能力，而视觉语言导航需要利用环境信息做出动作决策。目前一些视觉语言大模型（VLM）也已经提出，这些模型在图文问答，开放物体检测等方面具有很强的能力，但是存在上下文长度短，难以处理多图输入的问题。在这一限制下，利用LLM结合目标检测模型或是视觉语言模型能够带来语言描述的环境和物体信息，但转化过程势必带来视觉信息的损失。
\end{itemize}

针对这些问题，本章将首先探索大预言模型的在视觉语言导航任务上的适配方法，随后将介绍基于大语言模型增强的方法，最后将介绍完全基于大语言模型的方法。

\section{大语言模型在视觉语言导航任务的适配}

第二章介绍了传统的视觉语言模型解决方案，传统方法训练一个端到端的模型输出导航策略。以DUET 模型为例，其包含视觉感知，图信息记录等功能，采用交叉注意力机制完成指令和视觉特征的对齐，采用图注意力来进行全局路径规划。在这一架构下，有两种方法可以利用大模型可以在对传统模型进行增强。

第一种方法是在提供高级指令的同时，利用LLM提供当前视点处的子指令，利用基础模型充当指令的执行器。这种方法不需要对原始结构做大的修改，生成指令后和原始指令拼接后即可指导基础模型的动作。该方法也存在问题，即子指令的质量难以衡量，需要通过导航轨迹的执行情况才能判断，此外，一条导航轨迹可能有若干子指令参与，难以量化多条子指令的效果。

第二种方法是利用大模型输出当前视点处的导航给选择，在基础模型输出下一步动作的同时，利用LLM在动作空间中做出选择，利用动态融合的方式对两个导航策略进行合并。这种方法使LLM独立决策导航过程，便于判断每一步的行动效果。

这两种辅助适配室内导航任务的工作都需要对LLM进行适配，以符合视觉导航的任务。在本节将介绍LLM在视觉语言导航的适配方法，包含示例学习，思维链提示和基于微调的方法三个部分。

\subsection{情景学习}

使LLM在视觉语言导航任务上适配，需要在提示词方面给LLM关于任务，角色和限制的提示。具体来说，应当提示LLM背景信息，包括视觉语言导航任务的定义，智能体在导航环境中允许的操作等，同时还需要在提示词中加入对输出格式的限制，便于后续的解析。

提供详细的指令提示有时并不能使LLM完全理解任务，更好的做法是使用情景学习方法，给LLM一些演示片段，作为演示上下文，在最后将提问和这段演示拼接起来，使得LLM生成和演示相关的答案。在LLM 适配语言导航任务的场景下，提供系统提示$S$，具身指令信息$I$，周围环境信息$E$，对应的动作或子指令为$A$，则使用提示的提示方法如式所示。

\begin{equation}
    prompt=S+(I+E+A)*n*+I+E
\label{icl}
\end{equation}

其中$+$操作代表字符串拼接操作，由指令-答案构成的演示重复n次，由于上下文长度限制，一般采用$n=1$。

\subsection{思维链提示}

考虑到导航任务包含多个步骤才能完成，因此给予LLM思维链提示，使高级的导航指令同LLM的指令生成子任务接地。思维链（Chain of Though）提示在 Wei 等人的一篇工作中提出，在输入问题时添加推理提示，能够有效提升LLM在算数，常识等问题上的表现。

在视觉语言导航任务中，接收到如“去二楼的卧室找到一个枕头”的高级指令，智能体应当构建这样的思维过程：去二楼$\rightarrow$寻找通向二楼的楼梯$\rightarrow$执行。提示示例的生成可以使用能力更强的LLM实现，也可以人工标注。思维链提示用式表示：

\begin{equation}
    prompt_{cot} = S+(I+E+CoT+A)*n+I+E
\label{cot}
\end{equation}

其中系统提示$S$，具身指令信息$I$，周围环境信息$E$，对应的动作或子指令为$A$，CoT 为思维链提示过程。

\subsection{基于微调的方法}

视觉语言导航任务需要丰富的领域知识，如室内房间布局，以及室内物体分布等。这些领域知识可以通过微调的方式迁移到LLM中，提升导航的效果。主流的微调方法有监督微调方法（Supervised Fine-Tuning），人类反馈强化学习方法（Reinforcement Learning from Human Feedback）。监督微调方法需要收集室内场景数据，，在LLM预测结果上计算交叉熵损失，调整权重实现。人类反馈强化学习方法需要收集偏好数据，对同一问题的若干回答进行排序，利用此排序结果训练一个奖励模型。利用奖励模型作为奖励函数，利用PPO等算法对模型进行强化学习微调。

微调参数的方法包括全参数微调和LoRA（Low-Rank Adaptation）等参数高效的微调方法。后者在LLM的权重中附加了低秩矩阵调整模型的权重，减少了更新参数的数量，减少了计算负担，此外，LoRA保留了原始模型权重未作修改，能够保留微调前的知识。因此，微调时常使用LoRA作为参数调整的方法。

本节介绍了LLM在视觉语言导航任务上的三种适配方法。情景学习方法给LLM提供问答示例，思维链提示的适配方法给LLM思维引导，这两种方法都没有改变LLM的参数，而是使用合适的提示引导LLM 的回答。基于微调的方法改变参数，利用微调数据向LLM 添加室内导航知识。

\section{基于大语言模型增强传统模型的方法}

本节将介绍基于LLM增强传统视觉导航模型的方法。LLM预训练阶段学习了丰富的世界知识，而传统视觉语言导航模型，如DUET等，输入视觉特征和指令，利用记忆结构维护导航历史，能在视觉导航任务上取得较好的效果。二者优势互补，因此，将两种方式结合起来成为一种解决方案。

在本节将介绍 March in Chat 方法（MiC），该方法在 REVERIE 数据集上取得目前最好的效果。模型利用目标导向静态规划，房间物体场景感知器，场景动态规划三种方式，利用了LLM预训练过程的世界知识，指导传统模型完成导航任务。

利用LLM 对传统模型进行增强，需要解决LLM与传统模型的结合方式以及LLM介入时机两个问题。在前一节介绍了LLM的两种适配方式：利用LLM生成子指令和LLM直接生成视点选择，前者利用子指令提示补充高级指令，给出具体的指导，后者直接参与动作选择。MiC利用前一种方法，使用gpt2模型生成子指令，提升传统模型的效果。

具体而言，MiC 在一次导航过程中，首先利用LLM进行目标导向静态规划（GOSP）查询指令提到的目标物体，并推断物体可能所处的位置。这一步查询了LLM训练得到的世界知识。其次，智能体的房间物体场景感知器（ROASP）感知周围环境的视觉信息，并根据观察结果决定是否向LLM发出子指令生成请求。如果ROASP模块感知到周围环境变化，则场景导向动态规划模块（SODP）将会利用LLM生成新的细粒度导航子指令。重复上述过程，直至一次指令执行结束。

\textcolor{red}{方法（MiC）图}

目标导向静态规划模块（Goal-Oriented Static Planning ，GOSP）负责解析高级指令指向的物体，并推理目标位置。GOSP 模块分两次向LLM提出查询，第一次进行目标物体识别，第二次进行目标物体定位。第一次输入高级指令，要求LLM输出高级指令指代的物体。第二次输入查询到的物体，提示LLM输出在室内环境下可能所处的位置。该模块将提取先验知识的步骤显式分为了两个步骤进行，直接反映了LLM蕴含的世界知识。

房间物体场景感知器（Room-and-Object Aware Scene Perceiver，ROASP）负责感知室内场景变化和物体信息。该模块利用CLIP模型的图像和文本编码器，分别计算全景图像，房间类别和物体类别的特征向量，并利用余弦相似度进行视觉观察与场景匹配。在每一步，智能体接收周围的全景观察$R_t = \{r_i\}_{i=1}^n$ ，其中$r_i$ 为某个角度的观察信息，随后利用CLIP中的图像编码器计算图片特征$f_r$：

\begin{equation}
    f_r = E^{v}_{\text{CLIP}}(r_i)
\label{fr}
\end{equation}

利用Matterport3D 数据集中的房间类型标注，以及 REVERIE 数据集中提取出的对象类型标签，并将标签组装为房间类型描述$T_{room}$ 和 物体类型描述$T_{obj}$ ，利用CLIP的文本编码器计算相应的文本编码$f_{room}$ 和 $f_{obj}$:

\begin{equation}
    f_{room} = E^{t}_{\text{CLIP}}(T_{room})
\label{froom}
\end{equation}

\begin{equation}
    f_{obj} = E^{t}_{\text{CLIP}}(T_{obj})
\label{fobj}
\end{equation}

图像编码和文本描述编码过程将文本和图像信息投射到一个共同的语义空间中，两个向量之间的相似度越大，二者的语义距离越近。通过余弦相似度等指标可以度量全景视觉特征$f_r$ 和文本特征之间的相似度，实现视觉观察到文本内容的对齐。定义由特征向量计算出的房间分数$S_{room}$和物体分数$S_{obj}$：

\begin{equation}
    \begin{aligned}
        S_{\text {room }} & =\operatorname{Softmax}\left(f_{\text {room }} \cdot f_{\mathrm{r}}^{T}\right), \\
        S_{\text {obj }} & =\operatorname{Softmax}\left(f_{\text {obj }} \cdot f_{\mathrm{r}}^{T}\right) .
        \end{aligned}
\label{s}
\end{equation}

分数利用视觉特征和文本特征向量点成得到，利用Softmax 归一化得到最终分数。在所有视图中选择分数平均值最高的一类房间类型作为房型预测结果，选出分数最高的前几个物体作为对象观察结果。该模块实现了房间类型和周围物体的感知，为LLM提供了当前智能体所处的环境，反映了指令的执行阶段，为LLM生成下一步的指令提供依据。

ROASP 还模块控制了指令生成的时机，要求在房间类型发生变化的时候才向LLM请求生成子指令，有效降低了计算开销和误导的可能。一条子指令往往需要智能体移动若干次才能执行完成，若在每次移动时都产生子指令，可能导致子指令的频繁变化，带来不稳定的导航结果。此外，子指令可能出现误导内容，过高的子指令频率不利于传统模型纠偏。只有当房间类型变化时，代表智能体移动到新环境，此时借助LLM生成指导效果最佳。

子指令由场景导向动态规划模块（Scene-Oriented Dynamic Planning ，SODP）构建得到。SODP 的提示词分为三个部分，第一部分是 ROASP 中当前环境和物体的感知，第二部分是从 FG-R2R 数据集中选出的最相似示例，第三个部分是高级指令和先前生成的子指令。将三个部分拼接后输入到LLM中，生成子指令作为智能体提示。

SODP 中的示例匹配算法首先选择 FineGrained R2R 数据集作为演示集$D$，其中包含R2R数据集拆分出来的逐步指令$I_{low}$。利用当前的 REVERIE 高级指令作为查询 $Q$ ,每一个逐步指令$I_{low,i}$作为键$K$，利用Sentence-BERT 将查询$Q$ 和键$K_i$ 进行嵌入，计算相似性得分，最高的一条逐步指令$I_{low}$ 作为演示示例。选择最相关的演示提升了子指令的准确性和相关性，使生成指令靠近准确的答案。

MiC 这篇工作亮点在于创建通过子指令提示这一途径链接LLM和传统模型，结合了二者的优点，在平均带权路径长度（SPL）等指标上带来了较好的效果。该工作为本研究的后续尝试带来了启发。

\section{本章小结}
\textcolor{red}{本章小结}

\chapter{大语言模型视觉导航任务调优}

在前面的章节介绍了视觉语言导航任务的数据集，传统方法和基于LLM增强的方法。传统方法利用交叉注意力机制进行指令和视觉观察之间的对齐，使用图结构进行导航轨迹的记忆和回溯，在见过的环境下取得了较好效果，但是在未见环境下泛化能力不佳。LLM在预训练阶段学习了丰富的世界知识，使用incontext learning等方法可以引导LLM 回答问题相关的内容。但是LLM缺乏直接执行的能力，存在幻觉，以及难以直接感知视觉信息等缺点不利于视觉语言导航任务上的应用。March in Chat 方法首先提出了LLM和传统视觉语言导航模型结合的方法，启发了本研究。

MiC 方法存在以下几个问题：

子指令生成方面，采用gpt2模型生成子指令，该模型在 2019 年提出，至今性能已经落后。目前众多机构开源了不同参数规模和能力范围的LLM，包括LLama2，Mistral等，在性能上优于gpt2。更新的LLM采用了新的编码技术和质量更高的训练语料，利用更大的参数规模和能力。

最后，MiC 利用 CLIP 模型构建了室内场景和物体感知器，利用余弦相似度计算全景图像和文字概念之间的对齐程度，用于识别当前所处的房间和周围物体。这种转化过程可能导致信息丢失，利用图像描述等方法能够对观察图像的更多细节进行丰富，以提升对室内整体的感知。

为了解决这些问题，本研究尝试从子指令生成和视点选择两个方向，对LLM增强视觉语言导航的能力进行研究。

\section{LLM子指令生成调优}

MiC 等方法已经证明了使用子指令提示能够有效提升视觉语言导航任务的性能，在这个背景下，本研究希望在这个路径下继续改进和提升视觉语言导航模型的能力。为了利用子指令生成的方式提示智能体完成导航路径，本研究首先搭建了一个基础的实验框架，用于测试LLM与智能体的交互性能。随后开展了实验，测试直接生成子指令和微调后生成子指令的效果。最后分析了结果。

\subsection{LLM 子指令调优框架}

框架分为三个部分，执行器，判别器和LLM，如图所示。框架接受环境信息和高级指令作为输入，判别器判断是否采用LLM对智能体进行子指令提示，最终智能体充当执行器，利用子指令提示输出动作。重复此过程，直至一条导航指令执行结束。执行器利用预先训练好的传统模型（如DUET）等架构，具有基本的导航能力。判别器负责决定当前视点处是否需要LLM提供指令提示，判断输入是否满足子指令条件。LLM负责在收到判别器信号时生成子指令，利用其世界知识对导航任务做出指导。

\textcolor{red}{执行器的图片}

子指令和REVERIE 数据集中的高级指令直接拼接后作为执行器的输入。为了验证框架的正确性，探索子指令提示的能力上限，首先利用 FG-R2R 数据集中的子指令作为导航提示。

FG-R2R 由 R2R 数据集分割而来，和 REVERIE 共享相同的轨迹，但是 FG-R2R 采用细粒度的指导，而REVERIE 数据集给出最终目的的指导。FG-R2R 中的每条子指令$s_i$都和轨迹中的一段视点序列$V_i=\{v_1,v_2,\cdots,v_n\}$对齐，则判别器判断时间步$t$ 下智能体所处位置$v_t == v_{i,1}$  ，若满足，则直接采用$s_i$ 作为子指令提示。

为了表述方便，我们将FG-R2R 数据集中拆分出来的子指令$s$称作标准子指令，REVERIE 中的指令$I$为高级指令 。实验采用的模型是HM3D-DUET 预训练的模型，对比加入子指令$s$和未加入子指令（对照组）的效果。考虑到原始模型训练中并没有加入子指令进行微调，为此，我们采用和 HM3D-DUET 一致的超参数，加入标准子指令进行模型微调。在微调之后，模型的性能有了进一步的提升，如表\ref{table:single_column} 所示。


\begin{table}[htbp]
    \centering
    \caption{加入标准子指令提示的模型和未加提示的模型效果对比}
    \label{table:single_column}
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    \textbf{数据集分割} & \textbf{变量} & \textbf{路径长度} & \textbf{成功率（SR） (\%)} & \textbf{平均带权路径长度（SPL） (\%)} \\ \midrule
    \multirow{2}{*}{Val Train Seen} & 未加入$s$ & 9.91 & \textbf{100.00} & \textbf{95.84} \\
                                    & 加入$s$ & 10.86 & 96.75 & 91.37 \\ \midrule
    \multirow{2}{*}{Val Seen}       & 未加入$s$ & 14.82 & 62.97 & 54.76 \\
                                    & 加入$s$ & 15.81 & \textbf{76.67} & \textbf{65.83} \\ \midrule
    \multirow{2}{*}{Val Unseen}     & 未加入$s$ & 21.59 & 52.60 & 39.24 \\
                                    & 加入$s$ & 19.41 & \textbf{58.56} & \textbf{44.47} \\
    \bottomrule
    \end{tabular}
\end{table}

预先训练好的导航模型在训练集上取得了百分之百的成功率，加入子指令影响了指令的分布，故带来性能的下降。在验证环境下，由于子指令补充了高级指令缺失的具体导航信息，因此起到了显著的指导作用。
% Uncomment to list all the entries of the database.
% \nocite{*}
加入子指令后，智能体经过关键视点处能够得到明确的指令提示，在这种情况下，智能体的下一步行动受高级指令和子指令共同约束，因此提升了导航性能。在这个角度上看，如果LLM能够生成和子指令贴近的导航轨迹，那么就能够有效提升智能体的导航性能。

此外，由于标准子指令提示仅仅在最优轨迹上提供，智能体一旦偏移最优轨迹，则无法获得标准子指令提示。若采用LLM生成子指令提示，则在条件满足时即可生成提示，性能还有进一步的提升空间。

\subsection{LLM 直接生成子指令实验}

在验证子指令能够达到的性能上限之后，本研究开始使用LLM作为提示生成器，利用MiC 中的 ROASP 作为指令生成判别器，并对比有子指令提示下的模型和无提示下的模型效果。

子指令生成需要为LLM提供具体的上下文信息。这些上下文信息包括了高级指令、当前房间以及可见物体、目标房间和目标物体以及当前的场景观察，共四个部分。下面将依次介绍这些内容的构成。

\begin{enumerate}
    \item 高级指令：为当前执行的高级指令，在一次执行过程中，高级指令保持不变。
    \item 当前房间与可见物体：复现了MiC 方法中的 ROASP。利用物体和房间的文本描述和全景图像观察来计算视点处可见的物体和当前房间的场景类别。具体做法是将利用CLIP中的图像和文本编码器，将图像和文本转为特征向量，利用余弦相似度来度量二者的对齐程度。
    \item 目标房间和物体：利用LLM中的自然语言处理能力，输入高级指令$I$，输出目标物体和目标所在的房间。LLM 接收结构化的请求，如：”已知高级指令$I$，请回答该指令中的目标物体是什么。“ ，LLM 输出指令中的目标物体 $obj_t$ 。随后构造目标房间请求，如：”已知目标物体$o_t$ ，请回答该目标物体可能位于什么地方。“，LLM 输出目标物体可能处于的房间类型$room_t$ 。
    \item 当前场景的观察：这个部分利用BLIP2模型生成全景图对应的文字描述信息。为了区分不同方向的观察信息，将环绕一周的全景图拆分为12个方向，每个方向提供60°的水平可视角度，这样得到12张拆分的全景观察图片$O=\{o_0,o_2,\cdots,o_{11}\}$。为了将全景观察转为文字描述，利用预训练的视觉语言模型BLIP2，为每一张视觉场景图片$o_i$生成对应的图像字幕$c_i$ ，如图所示。图像字幕$C$ 提供了当前房间的视觉表达，提供的信息比物体和房间类型信息更加具体和直接。
\end{enumerate}

\textcolor{red}{TODO：图像字幕图片}

我们将四个部分的信息组装为LLM的系统提示，LLM输出子指令提示$s$。加入子指令提示的效果和未加子指令提示的指标对比如表\ref{llms}所示。

\begin{table}[htbp]
    \centering
    \caption{加入LLM子指令提示和未加提示的模型效果对比}
    \label{llms}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    \textbf{数据集分割} & \textbf{实验} & \textbf{路径长度} & \textbf{SR (\%)} & \textbf{OSR (\%)} & \textbf{SPL (\%)} \\ \midrule
    \multirow{2}{*}{Val Train Seen} & 未加入提示 & 9.91 & \textbf{100.00} & \textbf{100.00} & \textbf{95.84} \\
                                    & 加入LLM提示 & 11.59 & 95.12 & 98.37 & 86.94 \\ \midrule
    \multirow{2}{*}{Val Seen}       & 未加入提示 & 14.82 & 62.97 & 64.72 & 54.76 \\
                                    & 加入LLM提示 & 16.71 & \textbf{71.40} & \textbf{75.12} & \textbf{58.76} \\ \midrule
    \multirow{2}{*}{Val Unseen}     & 未加入提示 & 21.59 & 52.60 & 57.46 & \textbf{39.24} \\
                                    & 加入LLM提示 & 21.11 & \textbf{53.65} & \textbf{60.32} & 37.66 \\
    \bottomrule
    \end{tabular}
\end{table}

在加入提示之后，基础模型在加入子指令提示之后，模型在训练场景下的指标都有下降，这是由于LLM生成的子指令存在幻觉等问题，让基础模型的执行出现了误导。但是在见过场景下，和未见场景下，智能体执行的成功率和预言成功率都得到提升。这是由于LLM在未见场景下，能够在关键路径下为智能体的下一步动作提出指导，这种能力已经在LLM的预训练阶段具备，目前需要解决的问题是提升LLM子指令提示的质量，使之靠近标准子指令的输出内容与格式。

LLM生成子指令产生误导的原因在于，LLM在预训练过程中学习到的先验知识，和 VLN 任务中的室内房间布局，高级导航指令之间存在较大的差距。导致LLM无法理解导航所处的场景，以及子指令生成的任务。如图所示，LLM 在相同的环境下生成子指令和标准子指令之间存在较大差异，表现在指令格式和输出内容长度两个方面。

\textcolor{red}{LLM 生成的子指令和标准子指令对比}

为了解决这一问题，本研究将首先尝试对LLM进行微调，以适配子指令生成任务。

\subsection{LLM 子指令生成任务微调}


首先尝试了对LLM进行监督微调（Supervised Fine-Tuning，SFT），希望通过微调方式，将室内布局知识迁移到LLM中。为了节省计算资源，使用参数高效的LoRA方法对LLM进行微调。

进行监督微调的首要工作是准备微调的数据集。由于REVERIE 只提供了导航的最终目标，没有具体步骤，需要使用 R2R 数据集中的内容进行路径提示。为此，本研究利用 FG-R2R 数据集作为子指令样本，供训练使用。子指令由R2R中的细粒度指令拆分而来，每段子指令都和一段路径对齐，可以视作是高级指令的合理补充。

在取得子指令样本之后，还需要使用当前视点观察来辅助子指令生成。在模拟器环境中，视点观察由36个不同方向构成的离散图像构成。为了使语言模型理解周围环境，利用 BLIP2[16] 等视觉语言模型将视觉信息转换为文字。在数据集构造过程中，使用子指令对应的首个视点处的视觉观察，作为微调数据的一部分。最终微调数据集由三个部分构成：高级指令，当前场景观察，标准子指令。考虑到训练的时间成本，最终筛选了5000条数据作为微调数据集。

部分微调的数据样本如下图所示：

\textcolor{red}{TODO：微调数据格式}

为了控制微调的过程，需要寻找一种方法对微调后产生的子指令质量进行分析。最直接的方法是将微调后的模型接入基于学习的导航模型中（如DUET[4]），判断接入前后的效果变化。然而，该方法一次测试执行时间过长，并且在导航过程中，一条轨迹的生成可能是由若干子指令共同影响，从执行结果来看无法准确知晓单一生成子指令的质量。 为此，需要提出一种高效的子指令衡量方法。

考虑到子指令之间的相似度匹配，本质上还是对生成句子和答案之间的差异。两者之间的语义距离越近，说明指令生成的效果越好，因此研究采用MPNet，将大模型生成的子指令映射为向量，计算二者之间的余弦相似度作为微调结果的度量。余弦相似度度量了两个向量在方向上的相似性，其计算公式为两个向量的点积除以它们各自范数（或长度）的乘积。在该指标下挑选出生成效果较好的模型，用于后续实验。

定义预测出的子指令为$\hat{s}$，同一任务下对应位置下的标准子指令为$s$，用 MPNet 将两个子指令转换为向量 \( \text{MPNET}(\hat{s}) \) 和 \( \text{MPNET}(s) \)，则相似度 SCORE 可以表示为：

\begin{equation}
    \text{SCORE} = \frac{\text{MPNET}(\hat{s}) \cdot \text{MPNET}(s)}{\|\text{MPNET}(\hat{s})\| \times \|\text{MPNET}(s)\|}
\label{mps}
\end{equation}

其中，\( \text{MPNET}(\hat{s}) \cdot \text{MPNET}(s) \) 是两个向量的点积，而 \( \|\text{MPNET}(\hat{s})\| \) 和 \( \|\text{MPNET}(s)\| \) 分别是这两个向量的范数。这个公式确保了结果为 -1 到 1 之间的值，其中 1 表示完全相同的方向，-1 表示完全相反的方向，0 表示正交。

经过一轮微调之后，模型输出和标准子指令之间的相似度达到最大值，代表生成指令和标准子指令之间的语义距离已经拉近。

\textcolor{red}{余弦相似度变化表}

从直观上看，LLM也已经能生成和标准子指令近似的子指令，如表所示。

\textcolor{red}{TODO：生成前后的子指令}

为了衡量微调之后的模型生成子指令和微调前的子指令效果差异，我们利用REVERIE数据集中的所有数据进行一次实验，对比微调前后LLM生成子指令质量的变化情况。如表所示：

\begin{table}[htbp]
    \centering
    \caption{模型性能比较：加入提示后与微调后}
    \label{table:model_comparison}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    \textbf{数据集分割} & \textbf{实验} & \textbf{路径长度} & \textbf{SR (\%)} & \textbf{OSR (\%)} & \textbf{SPL (\%)} \\ \midrule
    \multirow{2}{*}{Val Train Seen} & 加入提示后 & 11.59 & \textbf{95.12} & \textbf{98.37} & \textbf{86.94} \\
                                    & 微调后 & 11.17 & 92.68 & 96.75 & 85.18 \\ \midrule
    \multirow{2}{*}{Val Seen}       & 加入提示后 & 16.71 & \textbf{71.40} & \textbf{75.12} & \textbf{58.76} \\
                                    & 微调后 & 16.88 & 69.57 & 73.58 & 57.37 \\ \midrule
    \multirow{2}{*}{Val Unseen}     & 加入提示后 & 21.11 & 53.65 & 60.32 & 37.66 \\
                                    & 微调后 & 20.77 & \textbf{55.04} & \textbf{60.86} & \textbf{38.99} \\
    \bottomrule
    \end{tabular}
\end{table}

微调提示加入之后，传统模型在未见数据下的准确率进一步提升，说明微调方法有效改进了指令生成的质量。虽然相比于未经过微调的方法而言，在已见环境下的模型导航效果有所下降，但是在未见环境下的导航能力有明显提升。但是相较于提供标准子指令的做法而言，LLM生成子指令提示的效果还是不佳，分析原因如下。

\begin{enumerate}
    \item 高级指令和子指令之间存在较大差距
    
    高级指令给出了目标最终导向的物体，需要智能体自行规划导航轨迹到相应的地点，而标准子指令由细粒度指令切分而来，包含高级指令不具备的详细路径提示。对于这种情况，LLM完成子指令的提示需要补充原本不具有的详细提示信息，生成一段子指令作为智能体的导航提示。

    在这个过程中LLM实际上需要分为几步进行。首先，LLM需要先理解高级指令的意图，知道高级指令指向的目标位置；其次，根据目标位置和当前的位置推断出可能的行动方向；最终，根据行动方向生成对应的子指令。这一思维过程是LLM在生成子指令时所欠缺的。

    \item 环境信息的精度不佳
    
    环境信息通过房间类型，房间物品类型，以及全景观察图像字幕构成。为了将这些视觉环境信息转为LLM适配的文字信息，使用了预训练的视觉语言模型完成这一工作，如CLIP，BLIP2 等。这些模型存在幻觉和检测精度不高的问题，存在一些导航中的关键物体和环境信息不能够准确的解析出来。例如，一条指令的目标是走到客厅中的玻璃门前，视点处的观察却没有出现关键的物体，这为子指令生成带来了困难。


    \item 任务困难
    
    子指令生成本质上来说是一个生成任务，要求生成一个字符序列以满足要求。考虑LLM的自回归性质，给定提示词$Q$ 生成子指令$s=[w_0,w_1,\cdots,w_n]$ 的概率为：
\[
P(w_1, w_2, \ldots, w_n | Q) = P(w_1|Q) \times P(w_2 \mid w_1) \times P(w_3 \mid w_1, w_2) \times \cdots \times P(w_n \mid w_1, \ldots, w_{n-1})
\]

子指令在庞大的搜索空间中寻找概率最大的组合。如果将子指令生成任务转化为选择任务，由于搜索空间的缩小，生成正确答案的难度将会大大降低。
\end{enumerate}

\section{利用视点选择做微调}

视点选择相较于子指令生成是一个更加简单的任务，我们对LLM直接生成视点选择作为一个新的尝试方向，在本节中，我们将进行直接生成视点选择，微调后的视点选择，以及思维链提示下的视点选择这三个部分的实验。实验之后，我们将讨论该方法的进一步改进方向。

直接生成下一步视点选择相较于生成子指令的方法降低了评估生成质量的难度。具体来说，子指令的评估过程需要在传统模型作为执行器的帮助下进行，而视点生成只需要判断是否走到下一个正确步骤即可。该方法还将子指令辅助这一生成式任务转为了视点选择这一选择任务，降低了LLM生成正确答案的难度。

具体而言，在视点选择方法的实验中，我们将LLM的生成内容由子指令改为了视点选择。给出当前智能体所处的位置$v$，以及可导航到的周围视点集合$N(v)$，LLM需要生成下一步移动的动作$\alpha \in N(v)$ ，对于每一个位置$v$，都存在一个最佳动作$\alpha*$ 。视点选择中的训练数据由REVERIE 的指令轨迹拆分而来，例如轨迹 $V_i=\{v_1,v_2,\cdots,v_n\}$ 可以拆分为$\{(v_t, v_{t+1}) \mid \forall t \in \{1, 2, \dots, n-1\}\}$ ，其中$v_{t+1}$ 作为$v_t$ 的下一步最佳动作。

衡量LLM生成动作预测的方法为准确度，在$N$ 条数据中，计算预测动作正确的比例：

\[
\text{score} = \frac{\sum_{i=1}^N \mathbf{1}(\hat{a}_i = a^*_i)}{N}
\]

这里，$\mathbf{1}(\hat{a}_i = a^*_i)$ 是一个指示函数，当预测的动作 $\hat{a}_i$ 与实际正确的动作 $a^*_i$ 相等时取值为 1，否则为 0。该公式通过计算所有数据中预测正确的比例来衡量模型的性能。

\textcolor{red}{TODO 衔接部分}

\subsection{直接利用LLM生成导航选择}

这里使用集合记号和条件语句来表达所有索引 \( t \) 从 1 到 \( n-1 \) 的视点对 \( (v_t, v_{t+1}) \)，这样的表示方法清晰表达了每一步的视点转移，直到轨迹的倒数第二个视点。

我们将上一节的提示词做了部分修改，以适配当前的视点选择任务。提示词包括系统提示，高级指令，目标物体及所在位置，当前位置以及可见主要物体，还有可选视点四个部分。

\begin{enumerate}
    \item 系统提示将子指令生成的任务修改为视点生成的任务，并对生成格式等做了要求。生成子指令只需在合适的位置截断LLM回答即可完成，视点选择则必须要求答案按照指定格式给出，以便在后续处理中进行答案解析。
    \item 高级指令，目标物体及所在位置，当前位置以及可见主要物体，这三个部分未作变化。
    \item 可选视点以”选项：观察信息“的格式给出。其中观察信息为智能体在当前视点$V$处，看向可选视点$N(V)$ 方向对应的图像字幕。
\end{enumerate}

改变之后的提示词示例如下：

\textcolor{red}{视点选择的提示词示例}

LLM在得到提示词之后，经过解析后获得当前视点的选择。由于计算资源所限，我们在 1000 条数据下做了测试。对比了随机猜测的准确率和LLM生成视点选择的准确率。测试结果如下：

\begin{table}[h]
    \centering
    \caption{数据集性能比较}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    数据集划分 & LLM准确率 & 随机猜测率 & LLM无效率 (invalid) \\ \midrule
    train & 0.307 & 0.267 & 0.007 \\
    val\_seen & 0.24 & 0.233 & 0.004 \\
    val\_unseen & 0.366 & 0.269 & 0.01 \\
    val\_train\_seen & 0.331 & 0.29 & 0.025 \\ \bottomrule
    \end{tabular}
    
\end{table}

LLM 直接生成子指令选择存在困难，表现为绝对准确率不高，只稍好于随机猜测。在不同的数据集划分中，候选视点数目有差异，导致不同数据集下的准确率存在差异，平均候选视点少的数据条目更容易猜测出正确答案。视点选择对LLM而言是一个新的任务，利用微调方法能加深LLM对于该任务的理解。

\subsection{微调后LLM生成导航选择}

为了提升生成效果，我们又收集数据进行微调，以提升预测准确率。选取了5000条数据做微调，微调提示部分保持不变，加入了正确动作$\alpha*$  和对应方向的视觉观察$c$ 作为计算损失的答案。模型在答案部分计算损失，采用LoRA方法附加权重，保持原始模型参数不变。在微调数据上进行两轮微调，取第一轮微调之后的结果。

\begin{table}[h]
    \centering
    \caption{微调前后数据集性能比较}
    \begin{tabular}{@{}lcccc@{}}
    \toprule
    数据集 & 微调前准确率  & 微调后准确率  & 微调前无效率 & 微调后无效率 \\ \midrule
    train          & 0.307 & \textbf{0.335} & 0.007 & \textbf{0.005} \\
    val\_seen      & 0.24  & \textbf{0.305} & 0.004 & \textbf{0.0}   \\
    val\_unseen    & 0.366 & \textbf{0.405} & 0.01  & 0.01  \\
    val\_train\_seen & 0.331 & \textbf{0.38}  & 0.025 & \textbf{0.0}   \\ \bottomrule
    \end{tabular}
    \end{table}

    可见微调后在四个划分的数据集上都获得了准确率的提升，同时无法解析的数据条目数量也有所减少。说明微调能够提升LLM生成视点选择方面的能力。然而，在准确率的相对精度上还是不高，本质上还是存在高级指令落地的困难。高级指令提供了宏观的指导，而LLM利用视点选择来执行，期间缺失的推理步骤并不能通过微调过程补齐。

 
\thesisbibliography{reference}

%
% Uncomment following codes to load bibliography database with native
% \bibliography command.
%
% \nocite{*}
% \bibliographystyle{thesis-uestc}
% \bibliography{reference}
%

\thesisaccomplish{publications}

\thesistranslationoriginal
\section{The OFDM Model of Multiple Carrier Waves}

\thesistranslationchinese
\section{基于多载波索引键控的正交频分多路复用系统模型}




\end{document}
